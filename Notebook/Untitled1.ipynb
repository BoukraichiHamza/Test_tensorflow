{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import des modules\n",
    "\"\"\"\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data =  [[0.4487857452076789], [0.8190119784963734], [0.9281564925175207], [0.7854812431426049], [0.34822596145768325]]\n",
      "Train_target =  [[4.9072743770186715], [10.289985467512555], [12.8001919332075], [9.622553507689984], [4.0132408361470855]]\n",
      "Test_data =  [[0.5193949864789646], [0.20820346884660923], [0.48978672208684493]]\n",
      "Test_target =  [[5.65159131128408], [3.0330057310006153], [5.326639889919498]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Generation des donnees\n",
    "\"\"\"\n",
    "eta_edp = 2\n",
    "def generate_data(n,m):\n",
    "    train_data   = []\n",
    "    train_target = []\n",
    "    test_data    = []\n",
    "    test_target  = []\n",
    "\n",
    "    #Generate train data\n",
    "    for i in range(1,n+1):\n",
    "        j = np.random.rand()\n",
    "        train_data.append([j])\n",
    "        f = eta_edp*np.exp(eta_edp*j)\n",
    "        train_target.append([f])\n",
    "    #Generate Test data\n",
    "    for i in range(1,m+1):\n",
    "        j = np.random.rand()\n",
    "        test_data.append([j])\n",
    "        f = eta_edp*np.exp(eta_edp*j)\n",
    "        test_target.append([f])\n",
    "\n",
    "    return train_data,train_target,test_data,test_target\n",
    "\n",
    "\"\"\"\n",
    "Test\n",
    "\"\"\"\n",
    "td,tt,td1,tt1 = generate_data(5,3)\n",
    "print('Train_data = ',td)\n",
    "print('Train_target = ',tt)\n",
    "print('Test_data = ',td1)\n",
    "print('Test_target = ',tt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "        Activation/Sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    \"\"\"\n",
    "        Derivative of the activation/Sigmoid\n",
    "\n",
    "    \"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "\n",
    "def ANN(x, v,w,b,d):\n",
    "    \"\"\"\n",
    "        Compute the output of the ANN\n",
    "        **input: **\n",
    "            *x : features\n",
    "            *v : first layer weights\n",
    "            *w : output node weight\n",
    "            *b : bias of first layer\n",
    "            *d : output node bias\n",
    "    \"\"\"\n",
    "    y = sigmoid(np.dot(v,np.transpose(x))+b)\n",
    "    print(np.shape(y))\n",
    "    return np.matmul(np.transpose(w), y) + d\n",
    "\n",
    "def dx_ANN(x, v,w,b,d):\n",
    "    \"\"\"\n",
    "        Compute the output of the derivated ANN\n",
    "        **input: **\n",
    "            *x : features\n",
    "            *v : first layer weights\n",
    "            *w : output node weight\n",
    "            *b : bias of first layer\n",
    "            *d : output node bias\n",
    "    \"\"\"\n",
    "    y = np.dot(v,np.transpose(x))+b\n",
    "    z = d_sigmoid(np.matmul(np.transpose(w), y) + d)\n",
    "    return np.dot(np.transpose(v),w)*z\n",
    "\n",
    "\n",
    "def jac_ANN(x,v,w,b,d):\n",
    "    \"\"\"\n",
    "        Compute the output of the Jacobian(weights) of the ANN\n",
    "        **input: **\n",
    "            *x : features\n",
    "            *v : first layer weights\n",
    "            *w : output node weight\n",
    "            *b : bias of first layer\n",
    "            *d : output node bias\n",
    "    \"\"\"\n",
    "    t = np.shape(x)[0]\n",
    "    s = np.shape(v)[0]\n",
    "    jac = np.zeros([t,3*s+1])\n",
    "    print(np.shape(jac))\n",
    "    for j in range(t):\n",
    "        jac[j][0:s] =  np.matmul(np.transpose(w),d_sigmoid(np.matmul(v,x[j])+b))\n",
    "        jac[j][s:2*s] = x[j]*jac[j][0:s]\n",
    "        jac[j][2*s:3*s] = np.reshape(sigmoid(x[j]*v+b),[s])\n",
    "        jac[j][3*s] = 1\n",
    "    return jac\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbiter =  4\n",
      "sol    =  [0.5  0.25]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Levenberg-Marquardt method for large scale nonlinear least squares problems \n",
    "    Input:\n",
    "        x     : variable with respect to which we solve for (weights)\n",
    "        y     : training sample\n",
    "        F     : function (-nabla ANN(u(y,x))-f(x))\n",
    "        J     : function handle that computes Jx and J'x\n",
    "        tol   : stopping tolerance \n",
    "        maxit : max number of iteration\n",
    "        \"\"\"\n",
    "\n",
    "def LM(x,y,F,J,tol,maxit,LAMBDA) :\n",
    "    \"\"\"\n",
    "    Initialisation\n",
    "    \"\"\"\n",
    "\n",
    "    # maximum, minimum  of LAMBDA\n",
    "    LAMBDA_max = 1.e+6\n",
    "    LAMBDA_min = 1.e-4\n",
    "    LAMBDAk    = LAMBDA\n",
    "\n",
    "    #Iterations counter\n",
    "    nbiter = 0\n",
    "\n",
    "    #Parameters\n",
    "    eta    = 0.1\n",
    "    gamma1 = 0.85\n",
    "    gamma2 = 1.5\n",
    "\n",
    "    #Iterate\n",
    "    xk = x\n",
    "\n",
    "    #Function, Jacobian and gradient evaluation\n",
    "    fx   = F(xk,y)\n",
    "    jac  = J(xk,y)\n",
    "    grad =  np.matmul(np.transpose(jac),fx)\n",
    "\n",
    "    #Actual norm of gradient\n",
    "    normgrad = np.linalg.norm(grad)\n",
    "\n",
    "    \"\"\" \n",
    "    Computation\n",
    "    \"\"\"\n",
    "    while (normgrad > tol) & (nbiter < maxit):\n",
    "        #inc iter\n",
    "        nbiter += 1\n",
    "\n",
    "        #Compute the step pk\n",
    "        n         = np.shape(jac)[1]\n",
    "        jacjac    = np.matmul(np.transpose(jac),jac)\n",
    "        jac_reg   = np.add(jacjac,np.dot(np.eye(n),LAMBDAk))\n",
    "        pk        = np.linalg.solve(jac_reg,np.dot(grad,-1))\n",
    "\n",
    "        #Compute rhok\n",
    "        xkaux = np.add(xk,pk)\n",
    "        ared  = np.linalg.norm(fx)**2 - np.linalg.norm(F(xkaux,y))**2\n",
    "        pred  = np.linalg.norm(fx)**2\n",
    "        pred  = pred - np.linalg.norm(np.add(fx,np.matmul(jac,pk)))**2\n",
    "        rhok  = ared/pred\n",
    "\n",
    "        #update iterate\n",
    "        if (rhok > eta):\n",
    "            xk      = xkaux\n",
    "            fx      = F(xk,y)\n",
    "            jac     = J(xk,y)\n",
    "            grad    = np.matmul(np.transpose(jac),fx)\n",
    "            LAMBDAk = gamma1*LAMBDAk\n",
    "            normgrad = np.linalg.norm(grad)\n",
    "        else:\n",
    "            LAMBDAk = gamma2*LAMBDAk\n",
    "    return xk,nbiter\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Tests\n",
    "\"\"\"\n",
    "\n",
    "A = np.array([[2,0],[0,4]])\n",
    "x = np.array([0.2,0.2])\n",
    "y = np.array([1,1])\n",
    "\n",
    "def Ftest(x,y):\n",
    "    Ax = np.matmul(A,x)\n",
    "    return np.add(Ax,np.dot(y,-1))\n",
    "\n",
    "def Jtest(x,y):\n",
    "    return A\n",
    "x,iters = LM(x,y,Ftest,Jtest,1e-6,1000,0.05)\n",
    "print('nbiter = ',iters)\n",
    "print('sol    = ',x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_variables(n):\n",
    "    \"\"\"\n",
    "        Init model variables (weights and bias)\n",
    "    \"\"\"\n",
    "    v = np.random.randn(n,1)\n",
    "    w = np.random.randn(n,1)\n",
    "    b = np.ones([n,1])\n",
    "    d = 0\n",
    "\n",
    "\n",
    "    return w,v,b,d\n",
    "\n",
    "def cost(predictions, targets):\n",
    "    \"\"\"\n",
    "        Compute the cost of the model\n",
    "        **input: **\n",
    "            *predictions: (Numpy vector) y\n",
    "            *targets: (Numpy vector) t\n",
    "    \"\"\"\n",
    "    return np.mean((predictions - targets) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable global\n",
    "n  = 10000 #Nombres de paquets de données générées\n",
    "p  = 1000 # Nombres de paquets de données de test générées \n",
    "m  = 200 #Nombres de neurones dans le hidden layer\n",
    "#Recuperation des donnees\n",
    "train_data,train_target, test_data, test_target = generate_data(n,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 2 5 1 4]\n",
      "(array([1, 2, 3]), array([4, 5, 6]), array([7, 8, 9]), array([10]))\n"
     ]
    }
   ],
   "source": [
    "def merge_weights(v,w,b,d):\n",
    "    return np.concatenate((v,w,b,d))\n",
    "\n",
    "def break_weights(weights,size):\n",
    "    v = weights[0:size]\n",
    "    w = weights[size:2*size]\n",
    "    b = weights[2*size:3*size]\n",
    "    d = np.array([weights[3*size]])\n",
    "    return v,w,b,d\n",
    "    \n",
    "\"\"\"\n",
    "Test\n",
    "\"\"\"\n",
    "print(merge_weights(np.array([1,2,3]),np.array([2,5]),np.array([1]),np.array([4])))\n",
    "print(break_weights(np.array([1,2,3,4,5,6,7,8,9,10]),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Method used to train the model using Levenberg Marquadt Method\n",
    "   **input: **\n",
    "           *features\n",
    "            *targets\n",
    "            *v : first layer weights\n",
    "            *w : output node weight\n",
    "            *b : bias of first layer\n",
    "            *d : output node bias\n",
    "    **return dw,dv,db,dd*\n",
    "            *update first layer weights\n",
    "            *update output node weight\n",
    "            *update first layer bias\n",
    "            *update output node weight\n",
    "\"\"\"\n",
    "def train(features, targets, w,v,b,d):\n",
    "    epochs = 100\n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    # Print current Accuracy\n",
    "    predictions = predict(features, weights, bias)\n",
    "    print(\"Accuracy = %s\" % np.mean(predictions == targets))\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Compute and display the cost every 10 epoch\n",
    "        if epoch % 10 == 0:\n",
    "            predictions = activation(pre_activation(features, weights, bias))\n",
    "            print(\"Current cost = %s\" % cost(predictions, targets))\n",
    "\n",
    "        # Appel Levenberg Marquadt\n",
    "        weights = merge_weights(w,v,b,d)\n",
    "        weights = LM(weights,features,F,J,1e-6,10000,0.05)\n",
    "\n",
    "    # Print current Accuracy\n",
    "    predictions = predict(features, weights, bias)\n",
    "    print(\"Accuracy = %s\" % np.mean(predictions == targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
